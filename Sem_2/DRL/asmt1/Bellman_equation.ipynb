{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RL asmt 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "598546f3a1896eb0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Discuss in detail how Markov Decision Process (MDP) can help in route planning application.\n",
    "Justify your answer in 350-400 words. Explain in detail your environment, state space, action\n",
    "space and equations aligned with the given problem. Write the answer in the Colab Cell itself.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70830942e3820ce6"
  },
  {
   "cell_type": "raw",
   "source": [
    "Markov property: Given we are in any of the states (each of the locations), the probablity of getting to the next station will only depend on A. the policy, B. The current location the customer is in, irrespective of the path he took to reach there, therefore satisfying the markov property.\n",
    "\n",
    "Bellman optimatlity solution for MDP: Clearly the problem can be broken into smaller subproblems and a) the optimal route from A to B is same as optimal route from A to S and S to B for some location S on the optimal route. also b) the subproblems repeat themseleves, meaning we can save solutions to subproblems and use it later to increase efficiency\n",
    "(Belman optimatlity equation can be used to get state action values.)\n",
    "\n",
    "Clearly, the problem can be modelled and solved as an MDP\n",
    "\n",
    "\n",
    "MDP definition:\n",
    "    State: Sequential, Episodic, finite state (we reach A to be in finite number of steps),fully observable enviorment. Each of the locations is a state (A to F)\n",
    "Actions: Given a state, we can choose one of the multiple routes leading out of it depending on out policy\n",
    "Rewards: For each location the driver is in, once he chooses a path he gets some negative reward, either in terms of cost incurred, time taken or traffic observed.\n",
    "State Transition probablity: Fullly observable deterministic enviorment. once we take an action in a state, the next state is determined by the node the route points to.\n",
    "Discounting Factor: Should be 1 as we want to optimise the total reward.\n",
    "\n",
    "Goal: stating state A, terminal state B, maximise the reward.\n",
    "\n",
    "Value iteration equation: v_k+1 = max(R_a + gamma* P_a * v_k)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "928145211f744cc6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fd987164d1a3bc57"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
