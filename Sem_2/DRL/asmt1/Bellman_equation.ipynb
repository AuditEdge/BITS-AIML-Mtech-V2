{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RL asmt 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "598546f3a1896eb0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Discuss in detail how Markov Decision Process (MDP) can help in route planning application.\n",
    "Justify your answer in 350-400 words. Explain in detail your environment, state space, action\n",
    "space  and equations aligned with the given problem. Write the answer in the Colab Cell itself.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70830942e3820ce6"
  },
  {
   "cell_type": "raw",
   "source": [
    "Markov property: Given we are in any of the states (each of the locations), the probablity of getting to the next station will only depend on A. the policy, B. The current location the customer is in, irrespective of the path he took to reach there, therefore satisfying the markov property.\n",
    "\n",
    "Bellman optimatlity solution for MDP: Clearly the problem can be broken into smaller subproblems and a) the optimal route from A to B is same as optimal route from A to S and S to B for some location S on the optimal route. also b) the subproblems repeat themseleves, meaning we can save solutions to subproblems and use it later to increase efficiency\n",
    "(Belman optimatlity equation can be used to get state action values.)\n",
    "\n",
    "Clearly, the problem can be modelled and solved as an MDP\n",
    "\n",
    "\n",
    "MDP definition:\n",
    "    State: Sequential, Episodic, finite state (we reach A to be in finite number of steps),fully observable enviorment. Each of the locations is a state (A to F)\n",
    "Actions: Given a state, we can choose one of the multiple routes leading out of it depending on out policy\n",
    "Rewards: For each location the driver is in, once he chooses a path he gets some negative reward, either in terms of cost incurred, time taken or traffic observed.\n",
    "State Transition probablity: Fullly observable deterministic enviorment. once we take an action in a state, the next state is determined by the node the route points to.\n",
    "Discounting Factor: Should be 1 as we want to optimise the total reward.\n",
    "\n",
    "Goal: stating state A, terminal state B, maximise the reward.\n",
    "\n",
    "Value iteration equation: v_k+1 = max(R_a + gamma* P_a * v_k)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "928145211f744cc6"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T17:47:35.975219800Z",
     "start_time": "2024-02-13T17:47:35.352360200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "    Route     Time     Cost Traffic Intensity Selection\n0       1     Best  Minimum            Medium       Yes\n1       2  Average  Minimum            Medium       Yes\n2       3     Best  Maximum               Low        No\n3       6    Worst  Maximum              High       Yes\n4       4  Average  Minimum              High        No\n5       1  Average  Minimum               Low       Yes\n6       5  Average  Maximum              High       Yes\n7       7    Worst  Minimum              High       Yes\n8       8     Best  Minimum            Medium        No\n9       2     Best  Maximum            Medium       Yes\n10      3    Worst  Maximum            Medium        No\n11      8     Best  Minimum               Low       Yes\n12      5    Worst  Maximum              High        No\n13      4    Worst  Minimum            Medium        No\n14      3     Best  Maximum               Low       Yes\n15      7  Average  Minimum               Low       Yes\n16      6  Average  Minimum               Low       Yes\n17      1  Average  Maximum               Low        No",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Route</th>\n      <th>Time</th>\n      <th>Cost</th>\n      <th>Traffic Intensity</th>\n      <th>Selection</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Best</td>\n      <td>Minimum</td>\n      <td>Medium</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Average</td>\n      <td>Minimum</td>\n      <td>Medium</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Best</td>\n      <td>Maximum</td>\n      <td>Low</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>Worst</td>\n      <td>Maximum</td>\n      <td>High</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Average</td>\n      <td>Minimum</td>\n      <td>High</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>Average</td>\n      <td>Minimum</td>\n      <td>Low</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>Average</td>\n      <td>Maximum</td>\n      <td>High</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>Worst</td>\n      <td>Minimum</td>\n      <td>High</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>Best</td>\n      <td>Minimum</td>\n      <td>Medium</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>Best</td>\n      <td>Maximum</td>\n      <td>Medium</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3</td>\n      <td>Worst</td>\n      <td>Maximum</td>\n      <td>Medium</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>8</td>\n      <td>Best</td>\n      <td>Minimum</td>\n      <td>Low</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>5</td>\n      <td>Worst</td>\n      <td>Maximum</td>\n      <td>High</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>4</td>\n      <td>Worst</td>\n      <td>Minimum</td>\n      <td>Medium</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>Best</td>\n      <td>Maximum</td>\n      <td>Low</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>7</td>\n      <td>Average</td>\n      <td>Minimum</td>\n      <td>Low</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>6</td>\n      <td>Average</td>\n      <td>Minimum</td>\n      <td>Low</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1</td>\n      <td>Average</td>\n      <td>Maximum</td>\n      <td>Low</td>\n      <td>No</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Reading the df and finding the prior, likelihood, predictor prior and posterior\n",
    "\n",
    "tmp = [[1, 'Best',    'Minimum', 'Medium', 'Yes'],\n",
    "       [2, 'Average', 'Minimum', 'Medium', 'Yes'],\n",
    "       [3, 'Best',    'Maximum', 'Low',    'No'],\n",
    "       [6, 'Worst',   'Maximum', 'High',   'Yes'],\n",
    "       [4, 'Average', 'Minimum', 'High',   'No'],\n",
    "       [1, 'Average', 'Minimum', 'Low',    'Yes'],\n",
    "       [5, 'Average', 'Maximum', 'High',   'Yes'],\n",
    "       [7, 'Worst',   'Minimum', 'High',   'Yes'],\n",
    "       [8, 'Best',    'Minimum', 'Medium', 'No'],\n",
    "       [2, 'Best',    'Maximum', 'Medium', 'Yes'],\n",
    "       [3, 'Worst',   'Maximum', 'Medium', 'No'],\n",
    "       [8, 'Best',    'Minimum', 'Low',    'Yes'],\n",
    "       [5, 'Worst',   'Maximum', 'High',   'No'],\n",
    "       [4, 'Worst',   'Minimum', 'Medium', 'No'],\n",
    "       [3, 'Best',    'Maximum', 'Low',    'Yes'],\n",
    "       [7, 'Average', 'Minimum', 'Low',    'Yes'],\n",
    "       [6, 'Average', 'Minimum', 'Low',    'Yes'],\n",
    "       [1, 'Average', 'Maximum', 'Low',    'No']]\n",
    "\n",
    "df = pd.DataFrame(tmp, columns = ['Route', 'Time', 'Cost', 'Traffic Intensity', 'Selection'])\n",
    "\n",
    "df.head(20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:43.041145800Z",
     "start_time": "2024-02-13T18:24:43.029745600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "prior = df['Selection'].value_counts(normalize=True).round(6)  # class prior\n",
    "predictor_prior = df['Route'].value_counts(normalize=True).round(6) # predictor prior\n",
    "likelihood_no = df[df['Selection']=='No']['Route'].value_counts(normalize=True).round(6).sort_index() # likelihood\n",
    "likelihood_yes = df[df['Selection']=='Yes']['Route'].value_counts(normalize=True).round(6).sort_index()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:43.277055500Z",
     "start_time": "2024-02-13T18:24:43.262681Z"
    }
   },
   "id": "fd987164d1a3bc57"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "Selection\nYes    0.611111\nNo     0.388889\nName: proportion, dtype: float64"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:43.520987600Z",
     "start_time": "2024-02-13T18:24:43.492717700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "Route\n1    0.166667\n3    0.166667\n2    0.111111\n6    0.111111\n4    0.111111\n5    0.111111\n7    0.111111\n8    0.111111\nName: proportion, dtype: float64"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_prior"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:43.729539800Z",
     "start_time": "2024-02-13T18:24:43.716305700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "Route\n1    0.181818\n2    0.181818\n3    0.090909\n5    0.090909\n6    0.181818\n7    0.181818\n8    0.090909\nName: proportion, dtype: float64"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_yes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:44.137370300Z",
     "start_time": "2024-02-13T18:24:44.125725600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "Route\n1    0.666667\n2    0.500000\n3    0.333333\n4    0.500000\n5    0.500000\n6    0.500000\n7    0.500000\n8    0.500000\nName: proportion, dtype: float64"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_posterior(class_likelihood, class_prior, pred_prior):\n",
    "    tmp = class_likelihood * class_prior  # class posterior = class likelihood * class prior / predictor prior\n",
    "    aligned_series1, aligned_series2 = tmp.align(pred_prior)\n",
    "    return (aligned_series1/aligned_series2).round(6)\n",
    "\n",
    "def normalise_posterior(a, b):\n",
    "    sum_series = a + b\n",
    "    return a/sum_series, b/sum_series\n",
    "\n",
    "posterior_yes_ = find_posterior(likelihood_yes , prior['Yes'], predictor_prior).fillna(1)\n",
    "posterior_no_ = find_posterior(likelihood_no , prior['No'], predictor_prior).fillna(1)\n",
    "\n",
    "posterior_yes, posterior_no = normalise_posterior(posterior_yes_, posterior_no_)\n",
    "posterior_yes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:44.584868200Z",
     "start_time": "2024-02-13T18:24:44.573830400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "Route\n1    0.333333\n2    0.500000\n3    0.666667\n4    0.500000\n5    0.500000\n6    0.500000\n7    0.500000\n8    0.500000\nName: proportion, dtype: float64"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior_no"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:45.182389800Z",
     "start_time": "2024-02-13T18:24:45.169375500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "State transition probability P(St+1=S1 | St = S0, At=a).\n",
    "In our case can be written as P(St+1=S1 | St = S0)  [State x State Matrix]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "{'A': {'A': 0, 'B': 0, 'C': 0.444445, 'D': 0.222222, 'E': 0, 'F': 0.333333},\n 'B': {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0, 'F': 0},\n 'C': {'A': 0, 'B': 1, 'C': 0, 'D': 0, 'E': 0, 'F': 0},\n 'D': {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 1, 'F': 0},\n 'E': {'A': 0, 'B': 1, 'C': 0, 'D': 0, 'E': 0, 'F': 0},\n 'F': {'A': 0, 'B': 0, 'C': 0.5, 'D': 0, 'E': 0.5, 'F': 0}}"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P(Selection=yes|R1)/( P(yes|R1) + P(yes|R2)+ P(yes|R3) ) for state A to C.\n",
    "Pca_ = (posterior_yes[1]/(posterior_yes[1]+posterior_yes[2]+posterior_yes[3])).round(6)  # normalisation\n",
    "Pda_ = (posterior_yes[3]/(posterior_yes[1]+posterior_yes[2]+posterior_yes[3])).round(6)\n",
    "Pfa_ = (posterior_yes[2]/(posterior_yes[1]+posterior_yes[2]+posterior_yes[3])).round(6)\n",
    "\n",
    "Pcf_ = (posterior_yes[4]/(posterior_yes[4]+posterior_yes[6])).round(6)\n",
    "Pef_ = (posterior_yes[6]/(posterior_yes[4]+posterior_yes[6])).round(6)\n",
    "\n",
    "state_transition_matrix = {\n",
    "    'A' : {'A':0, 'B':0, 'C':Pca_, 'D':Pda_, 'E':0, 'F':Pfa_ },  # keys - initial, dict entries final\n",
    "    'B' : {'A':0, 'B':0, 'C':0, 'D':0, 'E':0, 'F':0 },\n",
    "    'C' : {'A':0, 'B':1, 'C':0, 'D':0, 'E':0, 'F':0 },\n",
    "    'D' : {'A':0, 'B':0, 'C':0, 'D':0, 'E':1, 'F':0 },\n",
    "    'E' : {'A':0, 'B':1, 'C':0, 'D':0, 'E':0, 'F':0 },\n",
    "    'F' : {'A':0, 'B':0, 'C':Pcf_, 'D':0, 'E':Pef_, 'F':0 }\n",
    "}\n",
    "\n",
    "state_transition_matrix  # probability of transitioning to state x from state y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:45.849882100Z",
     "start_time": "2024-02-13T18:24:45.826660700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wherever there is only one path, we pick it so transition probability is 1.\n",
    "Where there are multiple paths, we use normalised P(yes|route) to find state transition probabilities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "{'A': {'R1': 'C', 'R2': 'F', 'R3': 'D'},\n 'B': {},\n 'C': {'R7': 'B'},\n 'D': {'R5': 'E'},\n 'E': {'R8': 'B'},\n 'F': {'R4': 'C', 'R6': 'E'}}"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also keeping a mapping of inital_state->actions->final_state\n",
    "state_action_mapping = {'A':{'R1':'C', 'R2':'F', 'R3':'D'},\n",
    "                        'B':{},\n",
    "                        'C':{'R7':'B'},\n",
    "                        'D':{'R5':'E'},\n",
    "                        'E':{'R8':'B'},\n",
    "                        'F':{'R4':'C', 'R6':'E'}}\n",
    "\n",
    "state_action_mapping"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:46.560283400Z",
     "start_time": "2024-02-13T18:24:46.544482200Z"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "Reward calculation will change basis objective\n",
    "\n",
    "R(St=s, At=a) = R(At=a) since state is uniquely known given action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use E[Cost|RouteA] as Reward. Here cost can be switched with time taken or traffic or weighted average of these"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "    Route  Time  Cost  Traffic Intensity Selection\n0       1     5     4                  4       Yes\n1       2     3     4                  4       Yes\n2       3     5     1                  5        No\n3       6     1     1                  1       Yes\n4       4     3     4                  1        No\n5       1     3     4                  5       Yes\n6       5     3     1                  1       Yes\n7       7     1     4                  1       Yes\n8       8     5     4                  4        No\n9       2     5     1                  4       Yes\n10      3     1     1                  4        No\n11      8     5     4                  5       Yes\n12      5     1     1                  1        No\n13      4     1     4                  4        No\n14      3     5     1                  5       Yes",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Route</th>\n      <th>Time</th>\n      <th>Cost</th>\n      <th>Traffic Intensity</th>\n      <th>Selection</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>4</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>4</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>5</td>\n      <td>1</td>\n      <td>5</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>5</td>\n      <td>4</td>\n      <td>4</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>5</td>\n      <td>1</td>\n      <td>4</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>8</td>\n      <td>5</td>\n      <td>4</td>\n      <td>5</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>4</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>5</td>\n      <td>1</td>\n      <td>5</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_mapping = {'Best':5,'Average':3,'Worst':1}\n",
    "cost_mapping = {'Minimum':4, 'Maximum':1}\n",
    "traffic_mapping = {'Low':5, 'Medium':4,'High':1}\n",
    "\n",
    "df['Time'] = df['Time'].replace(time_mapping)\n",
    "df['Cost'] = df['Cost'].replace(cost_mapping)\n",
    "df['Traffic Intensity'] = df['Traffic Intensity'].replace(traffic_mapping)\n",
    "\n",
    "df.head(15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:47.399053600Z",
     "start_time": "2024-02-13T18:24:47.373361200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "Route\n1    3.666667\n2    4.000000\n3    3.666667\n4    2.000000\n5    2.000000\n6    2.000000\n7    2.000000\n8    5.000000\nName: Time, dtype: float64"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = df.groupby('Route').agg({'Time':'mean', 'Cost':'mean', 'Traffic Intensity':'mean'})\n",
    "reward_time = rewards['Time'].round(6)  # expected time based reward if action A is chosen\n",
    "reward_cost = rewards['Cost'].round(6)  # same for traffic and cost\n",
    "reward_traffic = rewards['Traffic Intensity'].round(6)\n",
    "reward_combined =  (0.33*reward_time + 0.33*reward_cost + 0.33*reward_traffic).round(6)\n",
    "# considering all factors with equal weightage\n",
    "\n",
    "reward_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:49.276695700Z",
     "start_time": "2024-02-13T18:24:49.255168800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "Route\n1    3.0\n2    2.5\n3    1.0\n4    4.0\n5    1.0\n6    2.5\n7    4.0\n8    4.0\nName: Cost, dtype: float64"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_cost"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:50.015538100Z",
     "start_time": "2024-02-13T18:24:50.002555300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "Route\n1    4.666667\n2    4.000000\n3    4.666667\n4    2.500000\n5    1.000000\n6    3.000000\n7    3.000000\n8    4.500000\nName: Traffic Intensity, dtype: float64"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_traffic"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:53.783422200Z",
     "start_time": "2024-02-13T18:24:53.765496100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "Route\n1    3.740\n2    3.465\n3    3.080\n4    2.805\n5    1.320\n6    2.475\n7    2.970\n8    4.455\ndtype: float64"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_combined"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:54.450657800Z",
     "start_time": "2024-02-13T18:24:54.434233Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "Route\n1   -1.0\n2   -1.0\n3   -1.0\n4   -1.0\n5   -1.0\n6   -1.0\n7   -1.0\n8   -1.0\nName: Traffic Intensity, dtype: float64"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_dummy = reward_traffic.copy()  # taking a dummy -1 per step reward to solve minimum step problem\n",
    "reward_dummy[:]=-1\n",
    "reward_dummy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:24:56.107886800Z",
     "start_time": "2024-02-13T18:24:56.088949100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for iteration 0, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 0, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 0, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 0, state C and action R7, next state transition prob 1 \n",
      " for iteration 0, state D and action R5, next state transition prob 1 \n",
      " for iteration 0, state E and action R8, next state transition prob 1 \n",
      " for iteration 0, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 0, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 0 is [21.22225, 100.0, 99.0, 49.0, 99.0, 24.0]\n",
      " for iteration 1, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 1, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 1, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 1, state C and action R7, next state transition prob 1 \n",
      " for iteration 1, state D and action R5, next state transition prob 1 \n",
      " for iteration 1, state E and action R8, next state transition prob 1 \n",
      " for iteration 1, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 1, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 1 is [43.000054999999996, 100.0, 99.0, 98.0, 99.0, 48.5]\n",
      " for iteration 2, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 2, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 2, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 2, state C and action R7, next state transition prob 1 \n",
      " for iteration 2, state D and action R5, next state transition prob 1 \n",
      " for iteration 2, state E and action R8, next state transition prob 1 \n",
      " for iteration 2, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 2, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 2 is [43.000054999999996, 100.0, 99.0, 98.0, 99.0, 48.5]\n",
      " for iteration 3, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 3, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 3, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 3, state C and action R7, next state transition prob 1 \n",
      " for iteration 3, state D and action R5, next state transition prob 1 \n",
      " for iteration 3, state E and action R8, next state transition prob 1 \n",
      " for iteration 3, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 3, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 3 is [43.000054999999996, 100.0, 99.0, 98.0, 99.0, 48.5]\n",
      " for iteration 4, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 4, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 4, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 4, state C and action R7, next state transition prob 1 \n",
      " for iteration 4, state D and action R5, next state transition prob 1 \n",
      " for iteration 4, state E and action R8, next state transition prob 1 \n",
      " for iteration 4, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 4, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 4 is [43.000054999999996, 100.0, 99.0, 98.0, 99.0, 48.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": "A     43.000055\nB    100.000000\nC     99.000000\nD     98.000000\nE     99.000000\nF     48.500000\nName: value, dtype: float64"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running value iteration script - time\n",
    "def value_iteration(reward_type):\n",
    "    num_iterations = 0\n",
    "    value_vector = pd.Series({'A':50, 'B':100, 'C':50, 'D':50, 'E':50, 'F':50 }, name='value') # starting with an inital value estimate\n",
    "    gamma = 1 # setting up a discounting factor\n",
    "    while num_iterations < 5:\n",
    "        updated_value = {}\n",
    "        for state in ['A', 'B', 'C', 'D', 'E', 'F']:\n",
    "            new_state_value = -10**12\n",
    "            for action in range(1, 9):\n",
    "                if 'R'+str(action) in list(state_action_mapping[state].keys()):\n",
    "                    # action belongs to the state, find next state\n",
    "                    next_state = state_action_mapping[state]['R'+str(action)]\n",
    "                    next_state_transition_probability = state_transition_matrix[state][next_state]\n",
    "\n",
    "                    new_state_value = max(new_state_value,\n",
    "                                          reward_type[action] + gamma * next_state_transition_probability * value_vector[next_state] )\n",
    "                    print(' for iteration {}, state {} and action {}, next state transition prob {} '.format(\n",
    "                        num_iterations, state, 'R'+str(action), next_state_transition_probability\n",
    "                    ) )\n",
    "                else:\n",
    "                    # this action dosent belongs to this state\n",
    "                    pass\n",
    "\n",
    "            #v_k+1 = max(R_a + gamma* P_a * v_k)\n",
    "            updated_value[state] = value_vector[state] if new_state_value == -10**12 else new_state_value\n",
    "\n",
    "        updated_value_vector = pd.Series(updated_value, name='value')\n",
    "        print('value vector at iteration {} is {}'.format(num_iterations, list(updated_value_vector)))\n",
    "        value_vector = updated_value_vector\n",
    "\n",
    "        num_iterations +=1\n",
    "    return value_vector\n",
    "\n",
    "\n",
    "final_value_vector = value_iteration(reward_type=reward_dummy)\n",
    "final_value_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:37:34.761053500Z",
     "start_time": "2024-02-13T18:37:34.734962700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for iteration 0, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 0, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 0, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 0, state C and action R7, next state transition prob 1 \n",
      " for iteration 0, state D and action R5, next state transition prob 1 \n",
      " for iteration 0, state E and action R8, next state transition prob 1 \n",
      " for iteration 0, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 0, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 0 is [25.888917, 100.0, 102.0, 52.0, 105.0, 27.0]\n",
      " for iteration 1, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 1, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 1, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 1, state C and action R7, next state transition prob 1 \n",
      " for iteration 1, state D and action R5, next state transition prob 1 \n",
      " for iteration 1, state E and action R8, next state transition prob 1 \n",
      " for iteration 1, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 1, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 1 is [49.00005699999999, 100.0, 102.0, 107.0, 105.0, 54.5]\n",
      " for iteration 2, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 2, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 2, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 2, state C and action R7, next state transition prob 1 \n",
      " for iteration 2, state D and action R5, next state transition prob 1 \n",
      " for iteration 2, state E and action R8, next state transition prob 1 \n",
      " for iteration 2, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 2, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 2 is [49.00005699999999, 100.0, 102.0, 107.0, 105.0, 54.5]\n",
      " for iteration 3, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 3, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 3, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 3, state C and action R7, next state transition prob 1 \n",
      " for iteration 3, state D and action R5, next state transition prob 1 \n",
      " for iteration 3, state E and action R8, next state transition prob 1 \n",
      " for iteration 3, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 3, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 3 is [49.00005699999999, 100.0, 102.0, 107.0, 105.0, 54.5]\n",
      " for iteration 4, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 4, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 4, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 4, state C and action R7, next state transition prob 1 \n",
      " for iteration 4, state D and action R5, next state transition prob 1 \n",
      " for iteration 4, state E and action R8, next state transition prob 1 \n",
      " for iteration 4, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 4, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 4 is [49.00005699999999, 100.0, 102.0, 107.0, 105.0, 54.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": "A     49.000057\nB    100.000000\nC    102.000000\nD    107.000000\nE    105.000000\nF     54.500000\nName: value, dtype: float64"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_value_vector = value_iteration(reward_type=reward_time)\n",
    "final_value_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:37:34.989915Z",
     "start_time": "2024-02-13T18:37:34.970613400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for iteration 0, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 0, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 0, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 0, state C and action R7, next state transition prob 1 \n",
      " for iteration 0, state D and action R5, next state transition prob 1 \n",
      " for iteration 0, state E and action R8, next state transition prob 1 \n",
      " for iteration 0, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 0, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 0 is [25.22225, 100.0, 104.0, 51.0, 104.0, 29.0]\n",
      " for iteration 1, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 1, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 1, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 1, state C and action R7, next state transition prob 1 \n",
      " for iteration 1, state D and action R5, next state transition prob 1 \n",
      " for iteration 1, state E and action R8, next state transition prob 1 \n",
      " for iteration 1, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 1, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 1 is [49.22228, 100.0, 104.0, 105.0, 104.0, 56.0]\n",
      " for iteration 2, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 2, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 2, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 2, state C and action R7, next state transition prob 1 \n",
      " for iteration 2, state D and action R5, next state transition prob 1 \n",
      " for iteration 2, state E and action R8, next state transition prob 1 \n",
      " for iteration 2, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 2, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 2 is [49.22228, 100.0, 104.0, 105.0, 104.0, 56.0]\n",
      " for iteration 3, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 3, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 3, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 3, state C and action R7, next state transition prob 1 \n",
      " for iteration 3, state D and action R5, next state transition prob 1 \n",
      " for iteration 3, state E and action R8, next state transition prob 1 \n",
      " for iteration 3, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 3, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 3 is [49.22228, 100.0, 104.0, 105.0, 104.0, 56.0]\n",
      " for iteration 4, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 4, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 4, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 4, state C and action R7, next state transition prob 1 \n",
      " for iteration 4, state D and action R5, next state transition prob 1 \n",
      " for iteration 4, state E and action R8, next state transition prob 1 \n",
      " for iteration 4, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 4, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 4 is [49.22228, 100.0, 104.0, 105.0, 104.0, 56.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "A     49.22228\nB    100.00000\nC    104.00000\nD    105.00000\nE    104.00000\nF     56.00000\nName: value, dtype: float64"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_value_vector = value_iteration(reward_type=reward_cost)\n",
    "final_value_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:37:35.176695800Z",
     "start_time": "2024-02-13T18:37:35.163533300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for iteration 0, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 0, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 0, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 0, state C and action R7, next state transition prob 1 \n",
      " for iteration 0, state D and action R5, next state transition prob 1 \n",
      " for iteration 0, state E and action R8, next state transition prob 1 \n",
      " for iteration 0, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 0, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 0 is [26.888917, 100.0, 103.0, 51.0, 104.5, 28.0]\n",
      " for iteration 1, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 1, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 1, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 1, state C and action R7, next state transition prob 1 \n",
      " for iteration 1, state D and action R5, next state transition prob 1 \n",
      " for iteration 1, state E and action R8, next state transition prob 1 \n",
      " for iteration 1, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 1, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 1 is [50.444502, 100.0, 103.0, 105.5, 104.5, 55.25]\n",
      " for iteration 2, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 2, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 2, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 2, state C and action R7, next state transition prob 1 \n",
      " for iteration 2, state D and action R5, next state transition prob 1 \n",
      " for iteration 2, state E and action R8, next state transition prob 1 \n",
      " for iteration 2, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 2, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 2 is [50.444502, 100.0, 103.0, 105.5, 104.5, 55.25]\n",
      " for iteration 3, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 3, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 3, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 3, state C and action R7, next state transition prob 1 \n",
      " for iteration 3, state D and action R5, next state transition prob 1 \n",
      " for iteration 3, state E and action R8, next state transition prob 1 \n",
      " for iteration 3, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 3, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 3 is [50.444502, 100.0, 103.0, 105.5, 104.5, 55.25]\n",
      " for iteration 4, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 4, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 4, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 4, state C and action R7, next state transition prob 1 \n",
      " for iteration 4, state D and action R5, next state transition prob 1 \n",
      " for iteration 4, state E and action R8, next state transition prob 1 \n",
      " for iteration 4, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 4, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 4 is [50.444502, 100.0, 103.0, 105.5, 104.5, 55.25]\n"
     ]
    },
    {
     "data": {
      "text/plain": "A     50.444502\nB    100.000000\nC    103.000000\nD    105.500000\nE    104.500000\nF     55.250000\nName: value, dtype: float64"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_value_vector = value_iteration(reward_type=reward_traffic)\n",
    "final_value_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:37:35.360004500Z",
     "start_time": "2024-02-13T18:37:35.349866700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for iteration 0, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 0, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 0, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 0, state C and action R7, next state transition prob 1 \n",
      " for iteration 0, state D and action R5, next state transition prob 1 \n",
      " for iteration 0, state E and action R8, next state transition prob 1 \n",
      " for iteration 0, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 0, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 0 is [25.962249999999997, 100.0, 102.97, 51.32, 104.455, 27.805]\n",
      " for iteration 1, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 1, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 1, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 1, state C and action R7, next state transition prob 1 \n",
      " for iteration 1, state D and action R5, next state transition prob 1 \n",
      " for iteration 1, state E and action R8, next state transition prob 1 \n",
      " for iteration 1, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 1, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 1 is [49.50450165, 100.0, 102.97, 105.77499999999999, 104.455, 54.7025]\n",
      " for iteration 2, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 2, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 2, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 2, state C and action R7, next state transition prob 1 \n",
      " for iteration 2, state D and action R5, next state transition prob 1 \n",
      " for iteration 2, state E and action R8, next state transition prob 1 \n",
      " for iteration 2, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 2, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 2 is [49.50450165, 100.0, 102.97, 105.77499999999999, 104.455, 54.7025]\n",
      " for iteration 3, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 3, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 3, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 3, state C and action R7, next state transition prob 1 \n",
      " for iteration 3, state D and action R5, next state transition prob 1 \n",
      " for iteration 3, state E and action R8, next state transition prob 1 \n",
      " for iteration 3, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 3, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 3 is [49.50450165, 100.0, 102.97, 105.77499999999999, 104.455, 54.7025]\n",
      " for iteration 4, state A and action R1, next state transition prob 0.444445 \n",
      " for iteration 4, state A and action R2, next state transition prob 0.333333 \n",
      " for iteration 4, state A and action R3, next state transition prob 0.222222 \n",
      " for iteration 4, state C and action R7, next state transition prob 1 \n",
      " for iteration 4, state D and action R5, next state transition prob 1 \n",
      " for iteration 4, state E and action R8, next state transition prob 1 \n",
      " for iteration 4, state F and action R4, next state transition prob 0.5 \n",
      " for iteration 4, state F and action R6, next state transition prob 0.5 \n",
      "value vector at iteration 4 is [49.50450165, 100.0, 102.97, 105.77499999999999, 104.455, 54.7025]\n"
     ]
    },
    {
     "data": {
      "text/plain": "A     49.504502\nB    100.000000\nC    102.970000\nD    105.775000\nE    104.455000\nF     54.702500\nName: value, dtype: float64"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_value_vector = value_iteration(reward_type=reward_combined)\n",
    "final_value_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T18:37:35.538098800Z",
     "start_time": "2024-02-13T18:37:35.528158300Z"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "Best optimal route: Action that gives max reward + discounted next step value function\n",
    "\n",
    "Since reward at any time step is very small than final value functions, we only consider value functions for Final Route"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
